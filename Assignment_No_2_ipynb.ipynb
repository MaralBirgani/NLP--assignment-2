{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1z5HBpK8uKOLnHj5fSeJqd28FB2YTXOzx",
      "authorship_tag": "ABX9TyOsqltS63xjkKPWdIOA4L7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnahitaNouri/NLP-assignment-No.2/blob/main/Assignment_No_2_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3sBOdLjgLab",
        "outputId": "d6e70737-9e7c-4877-8e71-78af77b20b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-22.2.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-22.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "from faker import Faker\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "fake = Faker()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgcyzvN3gTUY",
        "outputId": "bba47703-d2d0-4160-c7dd-03d5e688d61e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates a random sentence using the Faker library.\n",
        "def generate_random_sentence():\n",
        "    return fake.sentence()\n",
        "\n",
        "# Creates a dummy text file with a specified size\n",
        "def create_dummy_text_file(file_path, file_size_mb):\n",
        "    print(f\"Creating a dummy text file with a size of {file_size_mb} MB...\")\n",
        "    target_size_bytes = file_size_mb * 1024 * 1024\n",
        "    current_size_bytes = 0\n",
        "\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        while current_size_bytes < target_size_bytes:\n",
        "            random_sentence = generate_random_sentence()\n",
        "            file.write(random_sentence + ' ')\n",
        "            current_size_bytes += len(random_sentence.encode('utf-8'))\n",
        "\n",
        "    print(\"Dummy text file creation completed.\")\n",
        "\n",
        "\n",
        "# Performs text preprocessing, including tokenization, stopword removal, and stemming\n",
        "def preprocess_text(text):\n",
        "    print(\"Performing text preprocessing:\")\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    porter = PorterStemmer()\n",
        "\n",
        "    # Tokenization\n",
        "    print(\" - Tokenizing...\")\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Stopword removal and stemming\n",
        "    print(\" - Removing stopwords and stemming...\")\n",
        "    filtered_words = [porter.stem(word) for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Joining the processed words\n",
        "    processed_text = \" \".join(filtered_words)\n",
        "\n",
        "    print(\"Text preprocessing completed.\")\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "# Calculates the cosine distance between two input texts\n",
        "def calculate_cosine_distance(text1, text2):\n",
        "    print(\"Calculating cosine distance:\")\n",
        "\n",
        "    # Preprocess both texts\n",
        "    text1 = preprocess_text(text1)\n",
        "    text2 = preprocess_text(text2)\n",
        "\n",
        "    # Vectorization\n",
        "    print(\" - Vectorizing texts...\")\n",
        "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
        "    vectors = vectorizer.toarray()\n",
        "\n",
        "    # Cosine similarity calculation\n",
        "    print(\" - Calculating cosine similarity...\")\n",
        "    similarity_matrix = cosine_similarity(vectors)\n",
        "    cosine_distance = similarity_matrix[0][1]\n",
        "\n",
        "    print(\"Cosine distance calculation completed.\")\n",
        "\n",
        "    return cosine_distance\n",
        "\n",
        "\n",
        "# Generates slices of input text based on specified criteria\n",
        "def generate_slices(input_text, context_window_size_mb=128, overlap_threshold=0.2):\n",
        "    print(\"Generating slices...\")\n",
        "\n",
        "    context_window_size = context_window_size_mb * 1024 * 1024  # Convert to bytes\n",
        "    input_size = len(input_text)\n",
        "\n",
        "    print(f\"Input Size: {input_size} bytes\")\n",
        "    if input_size <= context_window_size:\n",
        "        # Below the context window size, pass it as it is\n",
        "        print(\"Input is below context window size. Passing as it is.\")\n",
        "        return [input_text]\n",
        "\n",
        "    print(\"Input is above context window size. Generating slices...\")\n",
        "\n",
        "    slices = []\n",
        "    start = 0\n",
        "    end = context_window_size\n",
        "\n",
        "    while start < input_size:\n",
        "        slice_text = input_text[start:end]\n",
        "\n",
        "        if end < input_size:\n",
        "            next_start = end - int(overlap_threshold * context_window_size)\n",
        "            next_end = min(next_start + context_window_size, input_size)\n",
        "            next_slice_text = input_text[next_start:next_end]\n",
        "\n",
        "            distance = calculate_cosine_distance(slice_text, next_slice_text)\n",
        "\n",
        "            if distance <= overlap_threshold:\n",
        "                # Include the overlapping part in the next slice\n",
        "                slice_text = input_text[start:next_end]\n",
        "                start = next_end\n",
        "            else:\n",
        "                start = end\n",
        "        else:\n",
        "            # Last slice, include the remaining part\n",
        "            start = end\n",
        "\n",
        "        slices.append(slice_text)\n",
        "        print(f\"Generated Slice - Size: {len(slice_text)} bytes\", end='\\r')\n",
        "\n",
        "        end = min(start + context_window_size, input_size)\n",
        "\n",
        "    print(\"\\nSlice generation completed.\")\n",
        "    return slices\n",
        "\n",
        "\n",
        "# Saves slices to individual files in the specified directory\n",
        "def save_slices_to_files(slices, output_directory):\n",
        "    print(f\"Saving slices to files in the directory: {output_directory}...\")\n",
        "\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "\n",
        "    for i, slice_text in enumerate(slices):\n",
        "        print(f\"Saving Slice {i + 1} to file...\")\n",
        "        file_path = os.path.join(output_directory, f'slice_{i + 1}.txt')\n",
        "        with open(file_path, 'w+', encoding='utf-8') as file:\n",
        "            file.write(slice_text)\n",
        "\n",
        "    print(\"Slice saving completed.\")\n",
        "\n",
        "#usage\n",
        "file_path = '/content/drive/MyDrive/Dummy/dummy_file.txt'\n",
        "output_directory = '/content/drive/MyDrive/slices_output'\n",
        "file_size_mb = 130\n",
        "\n",
        "create_dummy_text_file(file_path, file_size_mb)\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    input_text = file.read()\n",
        "\n",
        "slices = generate_slices(input_text)\n",
        "print(\"\\nNumber of slices:\", len(slices))\n",
        "print(\"\\nSlices:\")\n",
        "for i, slice_text in enumerate(slices):\n",
        "    print(f\"Slice {i + 1} - Size: {len(slice_text)} bytes\")\n",
        "\n",
        "save_slices_to_files(slices, output_directory)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhvkndc5hYLz",
        "outputId": "bb12d1cb-583c-48f4-f910-a4462dbf963f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating a dummy text file with a size of 130 MB...\n",
            "Dummy text file creation completed.\n",
            "Generating slices...\n",
            "Input Size: 140101150 bytes\n",
            "Input is above context window size. Generating slices...\n",
            "Calculating cosine distance:\n",
            "Performing text preprocessing:\n",
            " - Tokenizing...\n",
            " - Removing stopwords and stemming...\n",
            "Text preprocessing completed.\n",
            "Performing text preprocessing:\n",
            " - Tokenizing...\n",
            " - Removing stopwords and stemming...\n",
            "Text preprocessing completed.\n",
            " - Vectorizing texts...\n",
            " - Calculating cosine similarity...\n",
            "Cosine distance calculation completed.\n",
            "Generated Slice - Size: 5883422 bytes\n",
            "Slice generation completed.\n",
            "\n",
            "Number of slices: 2\n",
            "\n",
            "Slices:\n",
            "Slice 1 - Size: 134217728 bytes\n",
            "Slice 2 - Size: 5883422 bytes\n",
            "Saving slices to files in the directory: /content/drive/MyDrive/slices_output...\n",
            "Saving Slice 1 to file...\n",
            "Saving Slice 2 to file...\n",
            "Slice saving completed.\n"
          ]
        }
      ]
    }
  ]
}